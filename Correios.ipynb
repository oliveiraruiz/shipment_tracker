#This is a test webcrawler I've been building it in a daily basis to train basic python and webcrawling skills.
#It is intended to automate the tedious task of checking if a package is going to arrive at your home
#This is applied to a brazilian mail company called Correios, but, with some tweaking, it can be used for any other one
#Please feel free to suggest any changes, this was made with learning purposes by a beginner
#It will surely need optmization

import requests
from bs4 import BeautifulSoup
from time import sleep
#These are the libraries I have chosen for this task, requests to gather web information, BeautifulSoup to format the data,
#and time to add the delay

delay = 600
#And there is the delay definition, 600s (10 minutes) is a good choice

codigo = str(input('Insira o c√≥digo de rastreamento: '))
#Asks for tracking code input

url = 'https://www.linkcorreios.com.br/?id=' + codigo
#Creates the url variable with the website where your tracking information is

r = requests.get(url)
#Gets the source code from the page and saves it in "r" variable

soup = BeautifulSoup(r.content, 'html.parser')
#Formats data in "r", to make it easier to search for information, insert print(soup) after this line to check how it is

filtered = soup.find('div', class_ = 'singlepost').find_all('ul', class_ = 'linha_status', limit = 1)
#Using the data in "soup", you then search for the div tag containing singlepost, then the tag ul with "linha_status"
#My choice was to get only the last status, for this I specified the limit to 1
#singlepost and linha_status were defined by me, by navigating in the "soup" data and finding the information that I want
#Everything I specified including the tags is saved in "filtered"

for ul in filtered:
    test1 = ul.text
#Gets the filtered data, and saves onlky the text to "test1". Tags are excluded from text this way

print(test1)
print()
print("I will keep you informed!")
#Prints the first status and some text from your choice to give that human touch

sleep(delay)
#This is important, this delay ensures the website is not flooded with requests, some companies may add a request limit by IP
#After the first request, it waits this time to do the next one, expecting a change between them

r = requests.get(url)
soup = BeautifulSoup(r.content, 'html.parser')
filtered = soup.find('div', class_ = 'singlepost').find_all('ul', class_ = 'linha_status', limit = 1)
#After the delay, we check the website again, this is a copy of what we did tthe first time

for ul in filtered:
    test2 = ul.text
#This second time we save the data into a different variable, for comparison

news = test1 == test2
#Here is the comparison between the first and second check

while news == True:
    sleep(delay)
    r = requests.get(url)
    soup = BeautifulSoup(r.content, 'html.parser')
    filtered = soup.find('div', class_ = 'singlepost').find_all('ul', class_ = 'linha_status', limit = 1)
    for ul in filtered:
        test2 = ul.text
#Until the comparison results in false, indicating a change, this loop keeps updating the "test2" value

print("There was an update!")
print()
print(test2)
#When a difference is noticed, these 3 lines are printed, showing the last status which triggered the change

#That's all for this first version, next step is working with datasets, stay tuned!
